## 实验一 云主机实现大数据实验

### 1.准备华为云环境

#### 1.1购买华为云ECS

按需计算，4台，鲲鹏通用计算增强型，2核4G

![image-20220111205042791](https://gitee.com/cpicture/picture-1/raw/master/202201112050885.png)

#### 1.2购买OBS

![image-20220111210109594](https://gitee.com/cpicture/picture-1/raw/master/202201112101684.png)

> OBS-myq8a6u2为实验二MRS使用的桶

**添加秘钥，并保存AK/SK**

![image-20220111210440170](https://gitee.com/cpicture/picture-1/raw/master/202201112104244.png)

![image-20220111210504980](https://gitee.com/cpicture/picture-1/raw/master/202201112105025.png)

### 2.搭建Hadoop集群

#### 2.1配置ECS

##### **SSH连接服务器**

<img src="https://gitee.com/cpicture/picture-1/raw/master/202201112108349.png" alt="image-20220111210841280" style="zoom:67%;" />

**服务器下载或本地上传‘’hadoop-2.8.3.tar.gz‘’、‘’OpenJDK8U-jdk_aarch64_linux_hotspot_8u191b12.tar.gz‘’和‘’hadoop-huaweicloud-2.8.3-hw-39.jar’‘**

``` 
wget https://archive.apache.org/dist/hadoop/common/hadoop-2.8.3/hadoop-2.8.3.tar.gz
```

``` 
wget https://github.com/AdoptOpenJDK/openjdk8-binaries/releases/download/jdk8u191-b12/OpenJDK8U-jdk_aarch64_linux_hotspot_8u191b12.tar.gz
```

![image-20220111211248239](https://gitee.com/cpicture/picture-1/raw/master/202201112112323.png)

![image-20220111211313722](https://gitee.com/cpicture/picture-1/raw/master/202201112113784.png)

![image-20220111211133579](https://gitee.com/cpicture/picture-1/raw/master/202201112111626.png)

##### 关闭防火墙

``` 
systemctl status firewalld 			#查看防火墙的状态：
systemctl stop firewalld			#关闭防火墙
systemctl disable firewalld			#禁止防火墙自启
```

![image-20220111212017196](https://gitee.com/cpicture/picture-1/raw/master/202201112120316.png)

##### 配置节点互信

node1~node4节点分别执行如下命令：

``` 
ssh-keygen -t rsa
```

提问框按默认连续回车即可，生成/root/.ssh/id_rsa.pub文件。

node1~node4节点分别执行命令cat /root/.ssh/id_rsa.pub命令。

```  yaml 
cat /root/.ssh/id_rsa.pub
```

![image-20220111213429926](https://gitee.com/cpicture/picture-1/raw/master/202201112134029.png)

将4个节点的内容拷贝汇总到一个文本中，再将该文本内容拷贝到node1、node2、node3、node4的/root/.ssh/authorized_keys中。

``` 
vim /root/.ssh/authorized_keys
```

![image-20220111213541191](https://gitee.com/cpicture/picture-1/raw/master/202201112135433.png)

node1~node4节点分别执行命令vim /etc/hosts，加入node1~node4对应IP及node节点名。

```
vim /etc/hosts
```

![image-20220111213659677](https://gitee.com/cpicture/picture-1/raw/master/202201112136840.png)

node1~node4节点分别执行命令ssh node1~node4，选择yes后，确保能够无密码跳转到目的节点。

```
ssh node2-node4		#node2为具体名字
```

![image-20220111213905281](https://gitee.com/cpicture/picture-1/raw/master/202201112139387.png)

#### 2.2安装OPJDK

 **创建目录**

node1~node4四个节点分别执行下列命令。

```
mkdir -p /home/modules/data/buf/
mkdir -p /home/test_tools/
mkdir -p /home/nm/localdir
```

**在node1节点，执行如下命令，将root目录下的jdk安装包拷贝到/usr/lib/jvm目录下。**

```
cp OpenJDK8U-jdk_aarch64_linux_hotspot_8u191b12.tar.gz /usr/lib/jvm/
```

**在node1节点，执行如下命令，将jdk安装包拷贝到其他几个节点，其中ecs-000为自己节点前面名称，可根据实际情况填写，i为节点数。**

```
 for i in {2..4};do scp /usr/lib/jvm/OpenJDK8U-jdk_aarch64_linux_hotspot_8u191b12.tar.gz root@node-000${i}:/usr/lib/jvm/;done
```

```
for i in {2..4};do scp /usr/lib/jvm/OpenJDK8U-jdk_aarch64_linux_hotspot_8u191b12.tar.gz root@ecs-39a9-000${i}:/usr/lib/jvm/;done
```

 **在node1~node4四个节点分别执行命令**

对目录下的OpenJDK进行解压

```
cd /usr/lib/jvm/
tar zxvf OpenJDK8U-jdk_aarch64_linux_hotspot_8u191b12.tar.gz
```

 **在node1~node4四个节点上编辑/etc/profile增加如下的配置**

```
 vim /etc/profile
```

**添加**

```
export JAVA_HOME=/usr/lib/jvm/jdk8u191-b12
```

![image-20220111214626628](https://gitee.com/cpicture/picture-1/raw/master/202201112146782.png)

**刷新全局变量，确认java版本**

```
source /etc/profile			#刷新全局变量
java -version
```

![image-20220111214915941](https://gitee.com/cpicture/picture-1/raw/master/202201112149025.png)

#### 2.3搭建Hadoop集群

**步骤1 登录node1节点，解压hadoop安装包** 

```
cp hadoop-2.8.3.tar.gz /home/modules/
cd /home/modules/
tar zxvf hadoop-2.8.3.tar.gz
```

**步骤2 配置hadoop环境变量**

```
vim /home/modules/hadoop-2.8.3/etc/hadoop/hadoop-env.sh
```

**编辑加入**

```
export JAVA_HOME=/usr/lib/jvm/jdk8u191-b12
```

**步骤3 配置hadoop core-site.xml配置文件**

执行命令：

```
vim /home/modules/hadoop-2.8.3/etc/hadoop/core-site.xml
```

参数配置（省略）

```

```

> ==fs.defaultFS、fs.obs.access.key、fs.obs.secret.key、fs.obs.endpoint需根据实际情况修改==

![image-20220113174445288](https://gitee.com/cpicture/picture-1/raw/master/202201131744660.png)**步骤 4    配置hdfs-site.xml**

node-0001节点执行下列命令：

```
vim /home/modules/hadoop-2.8.3/etc/hadoop/hdfs-site.xml
```

参数配置如下：

```
<configuration>
 <property>
  <name>dfs.replication</name>
  <value>3</value>
 </property>
 <property>
  <name>dfs.namenode.secondary.http-address</name>
  <value>node-0001:50090</value>
 </property>
 <property>
  <name>dfs.namenode.secondary.https-address</name>
  <value>node-0001:50091</value>
 </property>
</configuration>
```

> 注：主机名node-0001需要根据实际替换。

![image-20220113175018335](https://gitee.com/cpicture/picture-1/raw/master/202201131750548.png)

**步骤 5   配置yarn-site.xml**

node-0001节点执行下列命令：

```
vim /home/modules/hadoop-2.8.3/etc/hadoop/yarn-site.xml
```

参数配置省略

```
```

> 注：主机名node-0001需要根据实际情况修改。

![image-20220113175243040](https://gitee.com/cpicture/picture-1/raw/master/202201131752329.png)**步骤 6  配置mapred-sit.xml**

执行下列命令

```
cd /home/modules/hadoop-2.8.3/etc/hadoop/
mv mapred-site.xml.template mapred-site.xml
vim /home/modules/hadoop-2.8.3/etc/hadoop/mapred-site.xml
```

参数配置如下(node1为自己实际节点名)：

```
<configuration>
 <property>
  <name>mapreduce.framework.name</name>
  <value>yarn</value>
 </property>
 <property>
  <name>mapreduce.jobhistory.address</name>
  <value>node1:10020</value>
 </property>
 <property>
  <name>mapreduce.jobhistory.webapp.address</name>
  <value>node1:19888</value>
 </property>
 <property>
  <name>mapred.task.timeout</name>
  <value>1800000</value>
 </property>
</configuration>
```

**步骤 7    配置slaves**

node-0001节点执行下列命令：

```
vim /home/modules/hadoop-2.8.3/etc/hadoop/slaves
```

删除原有内容，添加内容如下：

```
node-0002
node-0003
node-0004
```

> node-00*需要根据实际更换

![image-20220113185635515](https://gitee.com/cpicture/picture-1/raw/master/202201131856734.png)**步骤 8   配置jar包**

上传jar包，然后copy到其他目录

```
cp hadoop-huaweicloud-2.8.3-hw-39.jar /home/modules/hadoop-2.8.3/share/hadoop/common/lib/
cp hadoop-huaweicloud-2.8.3-hw-39.jar /home/modules/hadoop-2.8.3/share/hadoop/tools/lib/
cp hadoop-huaweicloud-2.8.3-hw-39.jar /home/modules/hadoop-2.8.3/share/hadoop/httpfs/tomcat/webapps/webhdfs/WEB-INF/lib/  
cp hadoop-huaweicloud-2.8.3-hw-39.jar /home/modules/hadoop-2.8.3/share/hadoop/hdfs/lib/
```

 **步骤 9  分发hadoop包到各节点**

node1~node4四个节点下执行下列命令(node-000为自己节点前面名称，i为节点数，根据自己实际节点名进行编写)

```
for i in {2..4};do scp -r /home/modules/hadoop-2.8.3 root@ecs-39a9-000${i}:/home/modules/;done
```

**步骤 10   配置环境变量**

node1~node4四个节点下执行下列命令：

```
vim /etc/profile
```

添加如下：

```
export HADOOP_HOME=/home/modules/hadoop-2.8.3
export PATH=$JAVA_HOME/bin:$PATH
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
export HADOOP_CLASSPATH=/home/modules/hadoop-2.8.3/share/hadoop/tools/lib/*:$HADOOP_CLASSPATH
```

![image-20220113190916286](https://gitee.com/cpicture/picture-1/raw/master/202201131909495.png)

node1~node4四个节点下执行下列命令：

```
source /etc/profile			#刷新环境变量
```

**步骤 11   namenode初始化**

node1节点执行namenode初始化

执行下列命令：

```
hdfs namenode -format
```

> Namenode初始化只需要进行一次，多次初始化会导致 namenodeID和DataNode ID不对应
>
> 在运行Hadoop时会显示 ‘’0 datanode‘’

**步骤 12  启动HDFS**

node1节点执行命令：

```
start-dfs.sh
```

![image-20220113191432258](https://gitee.com/cpicture/picture-1/raw/master/202201131914510.png)**步骤 13   执行hdfs命令**

```
hdfs dfs -mkdir /bigdata
hdfs dfs -ls /
```

![image-20220113191535695](https://gitee.com/cpicture/picture-1/raw/master/202201131915796.png)

#### 2.4测试OBS互联

**步骤1  在OBS上传文件** 

![image-20220113191732748](https://gitee.com/cpicture/picture-1/raw/master/202201131917837.png)**步骤 2  执行hdfs命令查看OBS文件**

```
hdfs dfs -ls obs://obs-myq8a6u/
```

![image-20220113191913072](https://gitee.com/cpicture/picture-1/raw/master/202201131919327.png)

> **Hadoop集群与OBS互联成功。**

### 3.MapReduce存算分离

**步骤 1   node1节点启动YARN执行命令**

```
start-yarn.sh
```

![image-20220113192054914](https://gitee.com/cpicture/picture-1/raw/master/202201131920036.png)**步骤 2  测试文件**

**步骤 3  执行hadoop wordcount**

```
hadoop jar /home/modules/hadoop-2.8.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.3.jar wordcount obs://obs-myq8a6u/ /output
```

![image-20220113192248110](https://gitee.com/cpicture/picture-1/raw/master/202201131922247.png)

**Hosts报错**

![image-20220113192753296](https://gitee.com/cpicture/picture-1/raw/master/202201131927398.png)

> hosts中‘’127.0.0.1 hostname hostname‘’这一行需要删除

> 华为云服务器 /etc/hosts 文件在重启后会自动添加hostname和127.0.0.1的解析，导致自行添加 的本地解析出现问题

>  解决方法：打开 /etc/cloud/cloud.cfg 文件，注释掉manage_etc_hosts配置项，然后手动删除 /etc/hosts 中的的解析  node1~node4节点分别执行命令ssh node1~node4，==选择yes后==，确保能够无密码跳转到目的节 点。node1节点无密码跳转到node2节点如下图，其余同理。 

**步骤 4   查看结果**

```
hdfs dfs -cat /output/part-r-00000
```

![image-20220113201811073](https://gitee.com/cpicture/picture-1/raw/master/202201132018283.png)

> MapReduce实现存算分离计算成功!

> stop-all.sh		#停止所有脚本

## 综合实验一 华为云鲲鹏BigDataPro集群搭建

### 1.准备华为环境

### 2.搭建Hadoop集群

> 以上都已经在实验一中完成

> 每次服务器关机后只需重新重新启动Hadoop就可以

```
start-dfs.sh
start-yarn.sh
```

### 3.Spark集群存算分离

#### 3.1搭建Spark集群

**步骤 1    获取spark安装包**

**node-0001节点下载Spark安装包**

```
cd /root
wget https://archive.apache.org/dist/spark/spark-2.3.0/spark-2.3.0-bin-without-hadoop.tgz
```

![image-20220113211142197](https://gitee.com/cpicture/picture-1/raw/master/202201132111290.png)**步骤 2   解压spark安装包**

**node-0001节点执行下列命令：**

**复制安装包到/home/modules目录下**

```
cp /root/spark-2.3.0-bin-without-hadoop.tgz /home/modules
cd /home/modules
```

解压安装包

```
tar -zxvf spark-2.3.0-bin-without-hadoop.tgz
mv spark-2.3.0-bin-without-hadoop spark-2.3.0
```

![image-20220113211650953](https://gitee.com/cpicture/picture-1/raw/master/202201132116043.png)

**步骤 3    配置spark jar包**

**在node-0001节点，复制jar包到spark/jar下**

```
cp /root/hadoop-huaweicloud-2.8.3-hw-39.jar /home/modules/spark-2.3.0/jars/
cp /home/modules/hadoop-2.8.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar /home/modules/spark-2.3.0/jars/
```

**步骤 4  配置spark配置文件**

**node-0001节点执行下列命令：**

```
cd /home/modules/spark-2.3.0/conf/
mv spark-env.sh.template spark-env.sh
vim spark-env.sh
```

在末尾添加

```
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.el7_7.aarch64
export SCALA_HOME=/home/modules/spark-2.3.0/examples/src/main/scala
export HADOOP_HOME=/home/modules/hadoop-2.8.3
export HADOOP_CONF_DIR=/home/modules/hadoop-2.8.3/etc/hadoop
export SPARK_HOME=/home/modules/spark-2.3.0
export SPARK_DIST_CLASSPATH=$(/home/modules/hadoop-2.8.3/bin/hadoop classpath)
```

![image-20220113211412779](https://gitee.com/cpicture/picture-1/raw/master/202201132114003.png)**步骤 5   分发Spark**

**node-0001节点执行下列命令：**

```
scp -r /home/modules/spark-2.3.0/ root@node-0002:/home/modules/
scp -r /home/modules/spark-2.3.0/ root@node-0003:/home/modules/
scp -r /home/modules/spark-2.3.0/ root@node-0004:/home/modules/
```

**也可以使用循环**

```
for i in {2..4};do scp -r /home/modules/spark-2.3.0/ root@ecs-39a9-000${i}:/home/modules/;done
```

 **步骤6   配置环境变量**

**各节点执行：**

```
vim /etc/profile
```

添加如下内容：

```
export SPARK_HOME=/home/modules/spark-2.3.0
export PATH=${SPARK_HOME}/bin:${SPARK_HOME}/sbin:$PATH
```

保存退出

各节点执行如下命令，使环境变量生效：

```
source /etc/profile
```

![image-20220113212021001](https://gitee.com/cpicture/picture-1/raw/master/202201132120273.png)

### 3.2验证存算分离

**步骤 1   查看要计算的文件**

本次实验验证Spark与OBS实现存算分离，数据使用实验2.3.2中上传的playerinfo.txt文件。

 ![image-20220113212145192](https://gitee.com/cpicture/picture-1/raw/master/202201132121245.png)

**步骤 2    计算上述数据的wordcount**

**启动yarn**

```
start-yarn.sh
```

**启动pyspark**

```
[root@node-0001 conf]# pyspark
```

![image-20220113212351078](https://gitee.com/cpicture/picture-1/raw/master/202201132123351.png)

**输入下列代码：**

```
lines = spark.read.text("obs://obs-myq8a6u/").rdd.map(lambda r: r[0])
counts = lines.flatMap(lambda x: x.split(' ')).map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)
output = counts.collect()
for (word, count) in output:
	print("%s: %i" % (word, count))
```

![image-20220113212520006](https://gitee.com/cpicture/picture-1/raw/master/202201132125246.png)

![image-20220113212610840](https://gitee.com/cpicture/picture-1/raw/master/202201132126111.png)

> 输入exit()退出
>
> 类似于python的shell形式

## 实验二 云服务实现大数据

### 1.程序及数据准备

下载hadoop安装包

解压下载的安装包

在目录hadoop-3.1.3\share\hadoop\mapreduce中可以找到文件hadoop-mapreduce-examples-3.1.3.jar即为示例程序

> 在手册中已经给出

**创建OBS桶**

**新建‘’program‘’文件夹，将‘’hadoop-mapreduce-examples-3.1.3.jar‘’上传**

**新建‘’input‘’文件夹，将数据文件上传到input目录中**

![image-20220113213336240](https://gitee.com/cpicture/picture-1/raw/master/202201132133328.png)

![image-20220113213355031](https://gitee.com/cpicture/picture-1/raw/master/202201132133136.png)

### 2.搭建大数据环境

#### 2.1购买MRS服务

![image-20220111142800571](https://gitee.com/cpicture/picture-1/raw/master/202201111428764.png)

![image-20220111144245716](https://gitee.com/cpicture/picture-1/raw/master/202201111442773.png)

### 3.运行程序

#### 3.1创建作业

![image-20220111144606874](https://gitee.com/cpicture/picture-1/raw/master/202201111446973.png)

#### 3.2运行作业

![image-20220111144737371](https://gitee.com/cpicture/picture-1/raw/master/202201111447441.png)

#### 3.3查看输出

![image-20220111144841296](https://gitee.com/cpicture/picture-1/raw/master/202201111448374.png)

![image-20220111144931448](https://gitee.com/cpicture/picture-1/raw/master/202201111449492.png)

### 4.结果验证

#### 4.1 查看作业日志

![image-20220111145118887](https://gitee.com/cpicture/picture-1/raw/master/202201111451977.png)

### 5.组件管理

#### 5.1同步IAM账户

![image-20220111145217449](https://gitee.com/cpicture/picture-1/raw/master/202201111452493.png)

#### 5.2为MapReduce购买弹性公网IP

![image-20220111150206613](https://gitee.com/cpicture/picture-1/raw/master/202201111502703.png)

 **登录成功后进到MapReduce的Web管理界面，可以看到刚执行的MapReduce任务**

![image-20220111150809539](https://gitee.com/cpicture/picture-1/raw/master/202201111508604.png)

![image-20220111150918924](https://gitee.com/cpicture/picture-1/raw/master/202201111509023.png)

